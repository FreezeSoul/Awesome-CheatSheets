[![返回目录](https://parg.co/UCb)](https://github.com/wxyyxc1992/Awesome-CheatSheets)

# Crawler CheatSheet | 爬虫实践清单

反爬虫还有以下几种的目的：

1. 破坏了网站的正常访问。2. 保护数据。3. 防止商业竞争。

1. 限制 IP。
   当网站发现某个 IP 访问的速度不仅飞快，而且都是在同一个 IP 段的时候，会认为该 IP 在进行爬虫操作，因此会对该 IP 进行限制访问，拒绝传输数据。
   解决方法： 首先通过设置 User-Agent 来模拟正常的访问流程，同时通过获取代理 IP 来爬取数据。
1. 必须登录才能访问。
   我们在之前的文章中，通过设置 User-Agent 模拟浏览器，让服务器认为我们是一个正常的访问过程。当网站发现服务器压力非常大，而且都是正常的 User-Agent 时，往往会对部分比价重要的数据，设置为必须登录才能访问。
   解决方法： 我们可以首先注册一个账号，带上账号的 Cookie 爬取数据。
1. 使用验证码验证登录。
   网站使用验证码进行登录验证，查看网站必须在登录界面输入相应的验证码才能够访问。
   解决方法： 使用机器学习算法，对验证码进行训练。或者是通过打码平台以及人工打码的方式进行处理。

1. 使用异步加载的方式加载数据。
   网站的数据并不全部加载，只有当用户查看到最底部时，网站才会将剩下的数据加载出来。
   解决方法： 正如我们之前学到的一样，AJAX 技术的确难以处理。我们需要通过不断地请求，去分析其规律，根据规律模拟出 AJAX 所请求的网页链接。
1. 滑动滑块。
   这是目前比较流行的验证方法。即只有用户通过鼠标，将一部分的图像移动到对应的区域，才能够正常访问
   解决方法： 目前的解决方法，大部分是首先对图像进行处理，然后模拟人工滑动的方式。（难）
1. 发送错误信息。
   当网站识别出发出的请求是来自于爬虫程序时，并不会拒绝连接，而是为该请求提供一个错误的信息或者数据，一方面让爬虫程序误认为爬取到了想要的数据，另一方面，也降低了服务器的请求压力。
   解决方法： 抽取 50%或者以上的数据进行抽查或者再次爬取。
1. 行为识别。
   目前大型的网站，例如阿里巴巴等，会对用户在网站上操作的每一个动作进行判断。当用户的动作具有间歇性、鼠标停留时间等特征与正常访问时的数据产生误差时，系统会判定为爬虫程序，并且拒绝相应的请求。

解决方式： 通过 Selenium 和 PhantomJS 完全解决方式： 通过 Selenium 和 PhantomJS 完全模拟浏览器进行操作。

8. 重要数据图像化。
   网站在传输数据之前，将数据编码为图片的格式，通过图片来展示认为重要的数据。
   解决方式： 使用 OCR 识别技术，识别图像中的数据。（难）
