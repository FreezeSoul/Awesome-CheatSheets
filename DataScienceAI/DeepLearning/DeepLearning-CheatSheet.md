[![返回目录](https://parg.co/UCb)](https://github.com/wxyyxc1992/Awesome-CheatSheet)

# DeepLearning CheatSheet | 深度学习概念备忘

深度学习正在逐步地改变世界，从网络搜索、广告推荐这样传统的互联网业务到健康医疗、自动驾驶等不同的行业领域。百年前的电气革命为社会带来了新的支柱产业，而如今 AI 正是新时代的电力基础，驱动社会技术的快速发展。

对于支持向量机、Logistics 回归这样经典的机器学习算法而言，在数据量从零递增的初始阶段，其性能会不断提升；不过很快就会触碰到天花板，此时性能很难再随着数据集的增长而提升。而伴随着移动互联网时代的到来，我们能够从网站、移动应用或者其他安装在电子终端设备上的传感器中获取到海量的数据；这些数据在开启大数据时代的同时也为深度学习的发展提供了坚实的基础。我们在上图中也可以看出，越是大型的神经网络随着数据量的增加，其性能提升的越快，并且其性能天花板也是越高。

深度学习崛起的另一个重要基石就是计算能力的提升，这里不仅指新一代 CPU 或者 GPU 设备，还有是在许多基础优化算法上的革新，都使得我们能够更快地训练出神经网络。譬如早期我们会使用 Sigmod 函数作为神经网络的激活函数，随着 x 的增大其梯度会逐渐趋近于零，这就导致了模型收敛变得相对缓慢；而 ReLU 则能较好地避免这个问题，其在正无穷大时梯度值依然保持恒定。简单地从 Sigmod 函数迁移到 ReLU 即能够为模型训练带来极大的效率提升，这也方便了我们构建出更复杂的神经网络。

神经网络的分类很多，不过截止到目前大多数的有价值的神经网络都还是基于机器学习中所谓的有监督学习(Supervised Learning)。在有监督学习中，我们的训练数据集中已知了特征与结果输出之间的对应关系，而目标就是寻找正确的输入与输出之间的关系表示。譬如目前最赚钱的深度学习应用之一，在线广告中就是输入有关于网站展示的信息以及部分用户的信息，神经网络会预测用户是否会点击该广告；通过为不同的用户展示他们最感兴趣的广告，来增加用户的实际点击率。

# Neural Networks | 神经网络

神经网络其实是一个非常宽泛的称呼，它包括两类，一类是用计算机的方式去模拟人脑，这就是我们常说的 ANN（人工神经网络），另一类是研究生物学上的神经网络，又叫生物神经网络；在人工神经网络之中，又分为前馈神经网络和反馈神经网络这两种。神经网络由大量神经元组成。每个神经元获得线性组合的输入，经过非线性的激活函数，然后得到非线性的输出；线性结果加上值相关(Element wise)非线性结果，我们才能去模拟任意的复杂图形。

![image](https://user-images.githubusercontent.com/5803001/44627904-00824e00-a969-11e8-8524-f1f9ad40efad.png)

神经网络往往由多层堆叠而成，主要分为输入层(Input Layer)，隐层(Hidden Layer)以及输出层(Output Layer)这三种。输入层中的每个节点代表了数据集中的每条数据的某个特征，隐层中的每个节点则是接收上层传来的特征向量，将其与权重向量进行点乘，传入激活函数得到该节点对应的特征值；输出层则是将前隐层的输出转化为单一的输出值。

![image](https://user-images.githubusercontent.com/5803001/44627910-18f26880-a969-11e8-83b0-40ea1f682f2c.png)

在实际计算中，假设 $X$ 是 $N _ features_num$ 维度的数据集，即包含了 $N$ 条数据，每条数据包含了 $features_num$ 个特征。而隐层总的权重矩阵维度为 $features_num _ hidden*layer_nodes_num$，即每个隐层节点包含了形为 $features_num * 1$ 的权重向量；输入的数据特征向量与权重向量点乘后得到单一值。换言之，如果隐层共包含了 $hidden*layer_nodes_num$ 个节点，那么某条数据经过隐层变换后输入的数据就成了形为 $1 * hidden_layer_nodes_num$ 的特征向量。在包含偏置值(bais)的隐层中，每个节点的偏置值为 b，那么整个隐层的 偏置向量就是形为 $1 \times hidden_layer_nodes_num$ 的向量，与点乘得到的特征向量相加，得到某条数据经过该隐层后最终的特征向量。

```py
# 数据集
X = tf.placeholder(tf.float32, shape=(None,features_num))

# Weight, 权重矩阵
W = tf.Variable(tf.random_normal(shape=[features_num, hidden_layer_nodes_num]))

# 偏移量矩阵
B = tf.Variable(tf.ones(shape=([hidden_layer_nodes_num])))

# 构建神经网络计算图
XW = tf.matmul(X,W)

# Activation Function | 激活函数
A = tf.sigmoid(Z)
```

## 激活函数

![image](https://user-images.githubusercontent.com/5803001/44628032-42ac8f00-a96b-11e8-8072-f360af7814dc.png)

激活函数计算量大，反向传播求误差梯度时，求导涉及除法。反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0。

Softmax 函数则是用于多分类神经网络输出：

$$
\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}}
$$

![image](https://user-images.githubusercontent.com/5803001/44628064-d54d2e00-a96b-11e8-89e9-3c6cccebf039.png)

## 梯度下降

梯度下降法 Gradient Descent 是一种常用的一阶(first-order)优化方法，是求解无约束优化问题最简单、最经典的方法之一。考虑无约束优化问题$min_xf(x)$，其中$f(x)$为连续可微函数。如果能构造出一个序列$x^0,x^1,...,x^t$满足：

$$
f(x^{t+1}) < f(x^t),t=0,1,2...
$$

则不断执行该过程即可以收敛到局部极小点。而根据泰勒展示我们可以知道:

$$
f(x+\Delta x) \simeq f(x) + \Delta x^T \nabla f(x)
$$

于是，如果要满足 $f(x+\Delta x) < f(x)$，可以选择:

$$
\Delta x = -{step} \nabla f(x)
$$

其中$step$是一个小常数，表示步长。以求解目标函数最小化为例，梯度下降算法可能存在一下几种情况：

- 当目标函数为凸函数时，局部极小点就对应着函数全局最小值时，这种方法可以快速的找到最优解；
- 当目标函数存在多个局部最小值时，可能会陷入局部最优解。因此需要从多个随机的起点开始解的搜索。
- 当目标函数不存在最小值点，则可能陷入无限循环。因此，有必要设置最大迭代次数。

# CNN | 卷积神经网络

# RNN | 循环神经网络

循环神经网络最大的特点就在于循环神经网路的隐含层的每个节点的状态或输出除了与当前时刻的输入有关，还有上一个时刻的状态或者输出有关，隐含层节点之间存在循环连接。这使得循环神经网络具有对时间序列的记忆能力。
实际应用中最有效的序列模型称为门控 RNN（gated RNN）。包括基于长短期记忆（LSTM：long short-term memory）和基于门控循环单元(GRU：gated recurrent unit)的网络。门控 RNN 主要是解决一般 RNN 的梯度消失或者梯度爆炸的问题。

![image](https://user-images.githubusercontent.com/5803001/44517585-b09e4f80-a6fa-11e8-8177-407607d84fd7.png)

## LSTM

不同于普通的 RNN 节点单元，LSTM 引入了遗忘门（总共有 3 个门，输入们、遗忘门和输出们）用来决定我们需要从节点单元中抛弃哪些信息以及保留哪些信息。

![image](https://user-images.githubusercontent.com/5803001/44517720-05da6100-a6fb-11e8-9ffe-c018b9bf5baf.png)
